好,那今天這堂課呢,是要跟大家簡略的介紹 Diffusion model的基本概念。那其實 Diffusion model有很多不同的變形。那以下的說明主要來自最知名的Denoising Diffusion probabilistic model。大家常常提到的DDPM。 好,那今天其實比較成功的那一些用 Diffusion model做的影像生成的系統，比如說Dalle-E啊，Google的 ImageGen啊，或 Stable Diffusion啊，基本上都是用差不多的方法來作為他們的 Diffusion的 model。 好，那這個 Diffusion model是怎麼運作的呢？它運作的方法是這樣子，我們來看它是怎麼生成一張圖片的。在生成圖片的第一步，是你S sample一個都是雜訊的圖片。就是你 sample出一個從 Gaussian distribution裡面， sample出一個 vector。這個 vector裡面的數字，這個 vector的 dimension，跟你產生圖片大小是一模一樣的。假設你今天要生一張256乘256的圖片。從 normal distribution sample出來的那個 vector，它的 dimension就要是256乘256那麼多。你就把你 sample到的那個256乘256的 vector 排成一張圖片的樣子。 好，然後接下來呢，你就有一個 denoise的 module，denoise的 network。那等一下會講說這個 denoise 的 network 內部長什麼樣子。這個 denoise 的 network從它的名字裡面你就可以知道說，輸入一張都是雜訊的圖，那輸出它就會把雜訊濾掉一點。那你就會看到有一個貓的形狀。 然後呢，再做 denoise。那貓的形狀呢就逐漸出來。那你就 denoise 越做越多，越做越多。期待最終你就看到一張清晰的圖片。 那這個 denoise 的次數是事先定好的。那我們通常會給每一個 denoise 的步驟給它一個編號。那產生最終圖片的那個編號比較小，那從一開始從完全都是雜訊的輸入開始做 denoise的編號比較大。所以我們這邊就從1000、999，一直排到2、排到1。 那這個從雜訊到圖片的步驟啊，它叫做 reverse的 process。 那在概念上這件事情啊，其實就像是米開朗基羅說的，這個雕像呢其實本來就已經在大理石裡面。它只是把不要的部分拿掉。Diffusion model做的事情就是一樣的，本來圖片就已經在雜訊裡面。它只是把試作雜訊的部分把它濾掉，就產生一張圖片。 好，那以下呢，我本來是想要 Midjourney 把這句話的意境呈現出來。後來發現其實有點困難，我就直接把這句話呢，丟到 Midjourney 裡面，它就畫了一些雕像的半成品出來。就ChatGPT有一個專門把這個 prompt 改成 Midjourney 可以吃的 prompt 的咒語，然後生一下，看起來是這個樣子。看起來是不太行。然後後來我決定自己手動輸入。然後我輸入的句子是類似，這個有一個大衛像在石頭裡面。但是看起來 Midjourney 還是沒有很懂這個意境，它沒有辦法畫一個大衛像在石頭裡面的感覺這樣。 就我想說它可以畫一個有點透明的石頭，然後裡面有一個大衛像。那看起來我 prompt 的功力非常的弱，沒有辦法讓它畫這件事情。你如果看這個 Midjourney 的那個 Discord，裡面的人都畫出來的圖都讓我嘆為觀止，我說非常的羨慕。這每個人都是詠唱大師。 好，所以我詠唱的技巧還非常的不純熟，沒有辦法把我心中想要表達的意境畫出來。 好，那接下來呢，就要講這個 denoise 的 model 了。 好，那從這個圖上看來啊，你可能會想說這個 denoise 的 model 是不是同個呢？是不是同一個 denoise 的 model 反覆用很多次呢？是，我們這邊是把同一個 denoise 的 model 反覆進行使用。 但是因為在這邊每一個狀況你輸入的圖片差異非常大。在這個狀況你輸入的東西就是一個純雜訊。在這個狀況你輸入的東西雜訊非常小，它已經非常接近完整的圖。所以如果是同個模型它可能不一定能夠真的做得很好。所以怎麼辦呢？你這個 denoise 的 model 啊，它除了吃要被 denoise 的那張圖片以外，它還會多吃一個輸入。這個輸入代表現在 noise 嚴重的程度。然後1000代表剛開始 denoise 的時候，這個時候 noise 的嚴重程度很大。然後1代表說現在 denoise 的步驟快結束了，這是最後一步 denoise 的步驟，那顯然雜訊很小。那這個 denoise 的 module 希望它可以根據我們現在輸入在第幾個 step 的資訊，做出不同的回應。 好，這個是 denoise 的 model。所以我們確實只有用一個 denoise 的 model。但是這個 denoise 的 module 會吃一個額外的數字，告訴他說現在是在 denoise 的哪一個 step。 好，那 denoise 的 module 裡面實際內部做的事情是什麼呢？在 denoise 的模組裡面呢，它實際上會有一個 noise predictor。這個 noise predictor 做的事情就是去預測說在這張圖片裡面的雜訊長什麼樣子。這個 noise predictor 就吃這個要被 denoise 的圖片跟吃一個 noise 現在嚴重的程度，也就是我們現在進行到 denoise 的第幾個步驟的這個步驟的代號。然後呢，就輸出一張雜訊的圖。 好，它就是預測說在這張圖片裡面雜訊應該長什麼樣子。再把它輸出的雜訊，去減掉這個要被 denoise 的圖片，然後呢就產生 denoise 以後的結果。 喔，所以這邊 denoise 的 model 並不是輸入一張有 noise 的圖片，輸出就直接是 denoise 後的圖片。它其實是產生一個這個輸入的圖片的雜訊，再把雜訊扣掉輸入的圖片，來達到 denoise 的效果。 那你可能會想說，那為什麼要這麼麻煩呢？為什麼不直接 train一個 end-to-end 的 model，輸入是要被 denoise 的圖片，輸出就直接是 denoise 的結果呢？你可以這麼做，你可以這麼做。那我在文獻上也看過有人這麼做。不過現在多數的論文還是選擇 train一個 noise 的 predictor。因為可以想想看，產生一張圖片跟產生 noise，它的難度是不一樣的。如果你今天你的 denoise 的 module 可以產生一隻帶雜訊的貓，那它幾乎就可以說它已經會畫一隻貓了。 那所以要產生一個帶雜訊的貓跟產生一張圖片裡面的雜訊，這個難度是不一樣的。所以直接 train一個 noise predictor 可能是比較簡單的。train一個 end-to-end 的 model，要直接產生 denoise 的結果，是比較困難的。 好，那接下來的問題就是怎麼訓練這個 noise predictor 呢？我們已經知道我們一個 denoise 的 model 是一個 noisy 的 image，然後吃一個現在在 denoise 的 step 的數目，step 的 ID，然後產生 denoise 的結果。我有告訴你說，其實 denoise 的 model 裡面是一個 noise 的 predictor。它是要吃這張圖片，吃一個 ID，然後產生一個預測出來的雜訊。 但是你要產生出一個預測出來的雜訊，你得要有 ground truth 啊。我們在訓練 network 的時候，你就是要用 pair data 才能夠訓練啊。你需要告訴 noise predictor 這張圖片裡面的雜訊長什麼樣子，它才能夠學習怎麼把雜訊輸出出來啊。那這件事情怎麼做呢？怎麼製造出這樣子的資料呢？ 好，這個 noise predictor 啊，它的訓練資料是我們人去創造出來的。怎麼創造呢？它的創造方法是這樣。你從你的 database 裡面拿一張圖片出來，你自己加造影進去。你就 random 從 Gaussian distribution 裡面 sample 一組雜訊出來加上去。 產生有點 noisy 的 image。那你可能再 sample 一次，再得到更 noisy 的 image。以此類推，最後呢，整張圖片就看不出來原來是什麼東西。好，你把你的手上的有的照片呢都做這樣子的事情。這個加造影的過程啊，叫做 forward process。又叫做 diffusion 的 process。那做完這個 diffusion process 以後，你就有 noise predictor 的訓練資料了。怎麼說呢？對 noise predictor 來說，它的訓練資料就是這一張加完雜訊的圖片跟現在是第幾次加雜訊是 network 的輸入。而加入的這個雜訊就是 network 應該要 predict 的輸出，就是 network 輸出的 ground truth。 所以你在做完這個 diffusion 的 process 以後，你手上就有訓練資料了。你就告訴 noise predictor 說，看到這張圖，看到第二個 step 輸入2這個數字，你要輸出是什麼？你的 ground truth，就是一個長這個樣子的 noise。 就接下來就跟訓練一般的 network 一樣，train 下去，就結束了。 好，那但是我們要的不只是生圖而已。剛才講的好像是只是從一個雜訊裡面生出圖，還沒有把文字考慮進來。那要怎麼把文字考慮進來呢？那在下一頁投影片就會講怎麼把文字考慮進來。 但是在講怎麼把文字考慮進來之前，需要先讓大家知道的事情是，如果你今天要訓練一個影像生成的模型，它是吃文字，產生圖片。你其實還是需要成對的資料。你還是需要圖片跟文字成對的資料。 需要多少成對的資料呢？在我們的作業六裡面，我們其實沒有叫大家從文字生圖片，只有直接生圖片而已。那我們的資料是7萬張圖。那7萬張圖當然非常的少。 ImageNet 它是每一張圖片有一個類別的標記，還不是那個圖片的描述。不是像 a cat cat in the snow 這樣的圖片的描述。它只是每一張圖有一個標記，然後它有100萬張圖片。我們現在用這個圓圈的大小來代表圖片的數量。我告訴你今天這些你在網路會上看到的非常厲害的什麼 Midjourney 啊、Stable Diffusion 啊，或者是 Dalle-E 啊，它們的資料往往來自於 LAION 5B。LAION 有多少 image 呢？5.85 billion 的圖片。有58.5億張的圖片。所以如果你把圖片的數量換算成這個有顏色的圈圈的話，ImageNet 你可能以為已經很大了。LAION 有這麼大，有這麼多圖片。所以難怪今天這些模型可以產生這麼好的結果。那 LAION 其實有一個搜尋的 demo 的這個平台，你可以去裡面看看它裡面有什麼樣的圖片。那裡面真的是啥都有，啥都有。比如說貓的圖片呢，它不是只有貓的圖片跟英文文字的對應。它還有跟中文的對應，還有跟日文的對應。所以你就知道說為什麼今天那一些影像生成模型不是只看得懂英文。你給它中文它八成也都看得懂。那它它的訓練資料裡面也有中文跟其他的語言。 你可以試試看在裡面你找不找得到自己的照片，我是試一下我是找不到自己的照片啦。 蠻多名人的照片，比如說川普那個，隨便一找就一大堆。所以你也不用意外說為什麼那個 Midjourney 都畫得出川普。因為它就知道川普長什麼樣子。所以這個是你需要準備的訓練資料。 好，那有了這個文字跟影像成對的資料以後，我們先來看一下 denoise 的時候你是怎麼做的。denoise 的時候你的做法非常的簡單，把文字加到 denoise 的模組就結束了。所以你現在 denoise 的模組不是只看輸入的圖片做 denoise，它是根據輸入的圖片，加上一段文字的敘述，去把 noise 拿掉。所以在每一個 step，你的 denoise 的模組都會有一個額外的輸入，這個額外的輸入就是你要它根據這段文字的敘述生什麼樣的圖片。 好，那這個 denoise 的 module 裡面的 noise predictor 要怎麼改呢？你就是直接把這段文字給 noise predictor 就結束了。你就讓 noise predictor 多一個額外的輸入，也就是這段文字，就結束了。 好，那訓練的部分要怎麼改呢？你現在每一張圖片都有一段文字。所以今天你先把這張圖片做完 diffusion 的 process 以後，你再訓練的時候不只要給你的 noise predictor 加入雜訊後的圖片，還有現在 step 的 ID，多給一個就是文字的輸入。然後 noise predictor 會根據這三樣東西產生適當的 noise。就這樣。產生要去消掉的 noise，產生這個 ground truth。 這個就是 DDPM 完整演算法的 algorithm，從它論文原始論文裡面直接截出來的。 就這樣。沒有更多東西了。就這樣。然後但是這兩個 algorithm 裡面其實還是暗藏一些玄機啦。這個我們就留著下次再跟大家講。